{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226a26c1",
   "metadata": {},
   "source": [
    "## Use PromptTemplate to create a template for a string prompt\n",
    "By default, PromptTemplate use Python's str.format syntax for templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3379d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "#\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, AIMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c752b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3f90aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(openai_api_key=api_key,model=\"gpt-3.5-turbo-instruct\",temperature=0.5) #gpt-4-0125-preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045ce2d",
   "metadata": {},
   "source": [
    "### A simple string based Prompt formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a42e3fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "input_variables=['adjective', 'content'] template='Tell me a {adjective} joke about {content}'\n",
      "-----------------\n",
      "Tell me a stupid joke about Vietnamese\n",
      "-----------------\n",
      "\n",
      "\n",
      "Why did the Vietnamese man refuse to eat pho?\n",
      "\n",
      "Because he was already full of pho-klore!\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}\")\n",
    "prompt = prompt_template.format(adjective=\"stupid\",content=\"Vietnamese\")\n",
    "print(\"-----------------\")\n",
    "print(prompt_template)\n",
    "print(\"-----------------\")\n",
    "print(prompt)\n",
    "response = model.invoke(prompt)\n",
    "print(\"-----------------\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c35f00",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate: The prompt to chat model is a list of chat message\n",
    "Each chat message is associated with content and an additional parameter called role.\n",
    "\n",
    "For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human, or a system role.\n",
    "\n",
    "(Mỗi tin nhắn trò chuyện được liên kết với nội dung và một tham số bổ sung gọi là vai trò.\n",
    "\n",
    "Ví dụ: trong API hoàn thành trò chuyện OpenAI, tin nhắn trò chuyện có thể được liên kết với trợ lý AI, con người hoặc vai trò hệ thống.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ab69ab16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "input_variables=['name', 'user_input'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], template='You are a helpful AI bot. Your name is {name}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Hello, how are you doing?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"I'm doing well, thanks!\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))]\n",
      "-----------------\n",
      "[SystemMessage(content='You are a helpful AI bot. Your name is Bob'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='What is you name and what are you good at?')]\n",
      "-----------------\n",
      "\n",
      "AI: My name is Bob and I am an AI designed to assist and provide helpful information to users. I am programmed to be knowledgeable on a variety of topics and to continuously learn and improve my abilities. How can I assist you?\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful AI bot. Your name is {name}\"),\n",
    "        (\"human\",\"Hello, how are you doing?\"),\n",
    "        (\"ai\",\"I'm doing well, thanks!\"),\n",
    "        (\"human\",\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "prompt = chat_template.format_messages(name=\"Bob\",\n",
    "                                       user_input=\"What is you name and what are you good at?\"\n",
    "                                      )\n",
    "print(\"-----------------\")\n",
    "print(chat_template)\n",
    "print(\"-----------------\")\n",
    "print(prompt)\n",
    "response = model.invoke(prompt)\n",
    "print(\"-----------------\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ba644",
   "metadata": {},
   "source": [
    "### Various ways of formatting System/Human/AI prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4e087389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "input_variables=['text'] messages=[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more beat\"), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Give an answer in English and then translate the answer to Vietnamese'))]\n",
      "-----------------\n",
      "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more beat\"), HumanMessage(content=\"I don't like eating tasty things\"), AIMessage(content='Give an answer in English and then translate the answer to Vietnamese')]\n",
      "-----------------\n",
      "\n",
      "\n",
      "I'm not a fan of indulging in delicious treats. \n",
      "Tôi không thích ăn những thứ ngon miệng.\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to sound more beat\" # Bạn là một trợ lý hữu ích giúp viết lại văn bản của người dùng để nghe có nhịp điệu hơn\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "        AIMessagePromptTemplate.from_template(\"Give an answer in English and then translate the answer to Vietnamese\")\n",
    "    ]\n",
    ")\n",
    "prompt = chat_template.format_messages(text=\"I don't like eating tasty things\") #Tôi không thích ăn đồ ngon.\n",
    "print(\"-----------------\")\n",
    "print(chat_template)\n",
    "print(\"-----------------\")\n",
    "print(prompt)\n",
    "response = model.invoke(prompt)\n",
    "print(\"-----------------\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452fa67",
   "metadata": {},
   "source": [
    "### Providing a Context along with the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "47d33077",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\"\n",
    "# Context: Mô hình ngôn ngữ lớn (LLM) là những mô hình mới nhất được sử dụng trong NLP.\n",
    "# Hiệu suất vượt trội của chúng so với các mô hình nhỏ hơn đã khiến chúng cực kỳ hữu ích cho các nhà phát triển xây dựng các ứng dụng hỗ trợ NLP. \n",
    "# Bạn có thể truy cập các mô hình này thông qua thư viện `transformers` của Hugging Face, qua OpenAI bằng thư viện `openai` và qua Cohere bằng thư viện `cohere`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2ca6dcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face, OpenAI, and Cohere offer LLMs.\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f146427",
   "metadata": {},
   "source": [
    "### Langchain’s ```FewShotPromptTemplate``` caters to source knowledge input. The idea is to “train” the model on a few examples — we call this few-shot learning — and these examples are given to the model within the prompt.\n",
    "(FewShotPromptTemplate của Langchain phục vụ cho việc cung cấp nguồn kiến thức đầu vào. Ý tưởng là “đào tạo” mô hình trên một vài ví dụ - chúng tôi gọi đây là few-shot learning - và những ví dụ này được cung cấp cho mô hình trong prompt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c45fc1",
   "metadata": {},
   "source": [
    "The goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model.<br>\n",
    "(Mục tiêu của few-shot prompt templates là chọn động các ví dụ dựa trên thông tin đầu vào, sau đó định dạng các ví dụ trong final prompt để cung cấp cho mô hình.)\n",
    "\n",
    "<b>Fixed Examples</b><br>\n",
    "The most basic (and common) few-shot prompting technique is to use a fixed prompt example. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production. <br>\n",
    "\n",
    "The basic components of the template are: \n",
    "- examples: A list of dictionary examples to include in the final prompt. \n",
    "- example_prompt: converts each example into 1 or more messages through its format_messages method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "395bb58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "input_variables=['answer', 'query'] template='\\nUser: {query}\\nAI: {answer}\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "# create examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch\"\n",
    "    }\n",
    "]\n",
    "# create a eample template\n",
    "example_templates = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(input_variables=[\"query\",\"answer\"],\n",
    "                                template=example_templates\n",
    "                               )\n",
    "print(\"-----------------\")\n",
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7fc74c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now break our previous prompt into a prefix and suffix (Bây giờ hãy chia prompt trước đó thành tiền tố và hậu tố)\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the user's questions. \n",
    "Here are some examples: \n",
    "\"\"\"\n",
    "#Sau đây là đoạn trích từ cuộc trò chuyện với trợ lý AI. \n",
    "#Trợ lý này thường có tính châm biếm và hóm hỉnh, đưa ra những câu trả lời sáng tạo và hài hước cho các câu hỏi của người dùng. \n",
    "#Dưới đây là một số ví dụ:\n",
    "\n",
    "# the suffix is our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "18ebce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch\"}] example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nUser: {query}\\nAI: {answer}\\n') suffix='\\nUser: {query}\\n' example_separator='+++++++++++' prefix=\"The following are excerpts from conversations with an AI assistant. \\nThe assistant is typically sarcastic and witty, producing creative and funny responses to the user's questions. \\nHere are some examples: \\n\"\n"
     ]
    }
   ],
   "source": [
    "# Create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"+++++++++++\"\n",
    ")\n",
    "query = \"Who is the most stupid person in Vietnam?\"\n",
    "print(few_shot_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0507d597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are excerpts from conversations with an AI assistant. \n",
      "The assistant is typically sarcastic and witty, producing creative and funny responses to the user's questions. \n",
      "Here are some examples: \n",
      "+++++++++++\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do\n",
      "+++++++++++\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch\n",
      "+++++++++++\n",
      "User: Who is the most stupid person in Vietnam?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f76ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I'm an AI, I don't have the capability to determine someone's intelligence. But if you're asking for the most foolish decision, I'd say whoever thought it was a good idea to invade Russia in the winter.\n"
     ]
    }
   ],
   "source": [
    "# create chain\n",
    "chain = few_shot_prompt_template | model\n",
    "response = chain.invoke({\"query\":query})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
